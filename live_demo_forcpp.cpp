#include <vector>
#include <memory>
#include <string>
#include <fstream>

#include <inference_engine.hpp>
#include <opencv2/opencv.hpp>

using namespace InferenceEngine;

InferRequest make_InferRequest(std::string input_model_xml, std::string input_model_bin, std::string device){
    //推論インスタンスを作成する
    Core ie;
    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    CNNNetwork network = ie.ReadNetwork(input_model, input_model.substr(0, input_model.size() - 4) + WEIGHTS_EXT);
    network.setBatchSize(1);
    // --------------------------- 3. Configure input & output ---------------------------------------------
    // --------------------------- Prepare input blobs ---------------------------------
    InputInfo::Ptr input_info = network.getInputsInfo().begin()->second;

    std::string input_name = network.getInputsInfo().begin()->first;

    input_info->setLayout(Layout::NCHW);//TODOcheck!
    input_info->setPrecision(Precision::FP32);

    // --------------------------- Prepare output blobs ----------------------------------------------------
    DataPtr output_info = network.getOutputsInfo().begin()->second;
    std::string output_name = network.getOutputsInfo().begin()->first;

    output_info->setPrecision(Precision::FP32);

    ExecutableNetwork executable_network = ie.LoadNetwork(network, device);

    InferRequest infer_request = executable_network.CreateInferRequest();
    return infer_request;
}

//TODO Mat2data
inline void readRawFileFp64(const std::string& fileName, float* buffer, int inH, int inW, int inC)
{
    std::vector<double> temp(inH * inW * inC);

    std::ifstream file(fileName, std::ios::in | std::ios::binary | std::ios::ate);
    file.seekg(0, std::ios::end);
    int size = file.tellg();
    file.seekg(0, std::ios::beg);
    file.read((char*)(temp.data()), size);
    file.close();

    for (int itr = 0; itr < inH * inW * inC; itr++)
    {
        buffer[itr] = (float)temp[itr];
    }
}

void rawToBlob(const std::string rawFilePath, InferenceEngine::Blob::Ptr& blob)
{
    InferenceEngine::SizeVector blobSize = blob->getTensorDesc().getDims();
    const size_t width = blobSize[3];
    const size_t height = blobSize[2];
    const size_t channels = blobSize[1];
    float* blob_data = blob->buffer().as<float*>();

    std::vector<float> input(width * height * channels);
    readRawFileFp64(rawFilePath, input.data(), width, height, channels);

    for (int index = 0; index < width * height * channels; index++)
    {
        blob_data[index] = input[index];
    }
}


int main(){
    std::string input_model_xml = "model.xml";
    std::string input_model_bin = "model.bin";
    std::string device = "CPU";
    InferRequest infer_request = make_InferRequest(input_model_xml, input_model_bin, device)
    std::string input_image_path = "./test.jpg"
    cv::Mat image = cv::imread(input_image_path);
    cv::resize(image, image, cv::Size(input_info->getDims()[0], input_info->getDims()[1]));

    Blob::Ptr imgBlob = infer_request.GetBlob(input_name);
    rawToBlob(input_image_path, imgBlob);
}